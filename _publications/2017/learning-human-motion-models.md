---
ref: learning-human-motion-models
title: "Learning human motion models for long-term predictions"
authors: Partha Ghosh, Jie Song, Emre Aksan, Otmar Hilliges
date: 2017-10-10
venue: "2017 International Conference on 3D Vision (3DV)"
image: /assets/projects/learning-human-motion-models/teaser.png
external_project_page: 
video: https://files.ait.ethz.ch/projects/learning-human-motion-models/3dv_video.mp4
talk: 
paper: https://files.ait.ethz.ch/projects/learning-human-motion-models/3dv_learninghumanmotion.pdf
poster: 
data: 
code: https://bitbucket.org/parthaEth/humanposeprediction/overview
conference_url: http://3dv2017.science/3dv/
equal_contributions: 
award: "Best Paper Award"
bibtex: "@inproceedings{ghosh2017learning,
  title={Learning human motion models for long-term predictions},
  author={Ghosh, Partha and Song, Jie and Aksan, Emre and Hilliges, Otmar},
  booktitle={2017 International Conference on 3D Vision (3DV)},
  pages={458--466},
  year={2017},
  organization={IEEE}
}"
---


<img class="fullcol" src="/assets/projects/learning-human-motion-models/teaser_model.png" alt="Teaser-Picture" />

<p align="justify">
    <span class="figurecap">
        Schematic overview over the proposed method. (1) A variant of de-noising autoencoders learns the spatial configuration of the
        human skeleton via training with dropouts, removing entire joints at random which have to be reconstructed by the network. (2) We train
        a 3-layer LSTM recurrent neural network to predict skeletal configurations over time. (3) At inference time both components are stacked
        and the dropout autoencoder filters the noisy predictions of the LSTM layers, preventing accumulation of error and hence pose drift over
        time.
</p>
<hr />



<h3>Abstract</h3>
<p align="justify">
    We propose a new architecture for the learning of predictive
    spatio-temporal motion models from data alone. Our
    approach, dubbed the Dropout Autoencoder LSTM (DAE-LSTM),
    is capable of synthesizing natural looking motion
    sequences over long-time horizons<sup>1</sup> without catastrophic
    drift or motion degradation. The model consists of two components,
    a 3-layer recurrent neural network to model temporal
    aspects and a novel autoencoder that is trained to
    implicitly recover the spatial structure of the human skeleton
    via randomly removing information about joints during
    training. This Dropout Autoencoder (DAE) is then used
    to filter each predicted pose by a 3-layer LSTM network,
    reducing accumulation of correlated error and hence drift
    over time. Furthermore to alleviate insufficiency of commonly
    used quality metric, we propose a new evaluation
    protocol using action classifiers to assess the quality of synthetic
    motion sequences. The proposed protocol can be used
    to assess quality of generated sequences of arbitrary length.
    Finally, we evaluate our proposed method on two of the
    largest motion-capture datasets available and show that our
    model outperforms the state-of-the-art techniques on a variety
    of actions, including cyclic and acyclic motion, and
    that it can produce natural looking sequences over longer
    time horizons than previous methods.
</p>
<p>
    <sup>1</sup> > 10s for periodic motions, e.g. walking, > 2s for aperiodic motion, e.g. eating
</p>
<hr />
    


<h3>Video</h3>
<div class="video">
   <iframe width="864" height="486" src="https://www.youtube.com/embed/PgJ2kZR9V5w" frameborder="0" allowfullscreen></iframe>
</div>

    


<!-- <div class="fullcol">
    <h3>System overview</h3>
    <img class="fullcol" src="/assets/projects/puppet/repesentative_img_final.png" alt="Sys-Overview-Picture" />
    <div class="fullcol">
        <p align="left">
            <span class="figurecap">
                 Illustration of our pipeline from input character to fluid tangible animation using an optimized device configuration. The horse has 29 bones, controlled by 8 joints.
            </span>
        </p>
        <hr />
        <br/>
    </div>
</div>-->


<!--<div class="fullcol">
    <h3>Gallery</h3>
    <br/>
    <img class="fullcol" src="/assets/projects/puppet/gallery.png" alt="Gallery-Picture" />
    <p align="justify">
        <span class="figurecap">
            Depending on the available kit, device build instruction plans with different complexity are generated by our algorithm. Note that
the models have much higher degrees of freedom than the generated control structures. The inputs were (nr. bones/nr. sample poses): Horse:
(29/25 galloping, going up) â€“ Dragon: (110/12 flying, some walking); Scorpion (62/20 walking, attacking); Dancer (22/6). Note that the
device for the Dancer is asymmetric due to the asymmetry in the input poses: the left arm of the character moves almost rigidly with the torso
and it is thus not necessary to have any joint controlling the left arm.
        </span>
    </p>
    <hr />
</div>

<div class="fullcol">
    <h3>Acknowledgments</h3>
    <p align="justify">
We are grateful to C&eacute;dric Pradalier and Evgeni Sorkine for invalu-
able discussions and engineering support, to Sebastian Schoellham-
mer for his assistance on 3D modeling and rigging in Maya, to
Olga Diamanti for composing the accompanying video, to C&eacute;cile Edwards-Rietmann for narrating it and to Jeannine Wymann for her
help in assembling the prototypes. We also thank our
user study participants. This work was supported in part by the SNF grant
200021_162958 and the ERC grant iModel (StG-2012-306877). Alec Jacobson
is funded in part by NSF grants IIS-14-09286 and IIS-17257.
    </p>
    <hr />
    <br/>
    <br/>
</div> -->

